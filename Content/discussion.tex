%
\clearpage
\section{Discussion}

Our primary conceptual contribution is establishing that batch effects can be formalized as causal effects.  Given this realization, we propose a framework for quantifying and removing batch effects in high-dimensional datasets featuring non-Euclidean measurements. We propose a straightforward approach to augment existing strategies for proper batch effect correction and removal, by first prepending existing techniques using causal balancing procedures. We explore the advantages of these procedures by demonstrating that causal approaches (such as \cccombat~and Causal \Dcorr) yield empirically valid approaches to handling batch effects, or report that no inference can be made. This represents a much more principled approach to alternative strategies which simply introduce or presume potentially disastrous artifactual biases. Finally, our neuroimaging use-case shows the utility of the methods developed herein for estimating and removing batch-related effects from large biological datasets. Specifically, our methods identify different causal effects as compared to prior approaches, each of which identifies features similar to a completely unadjusted approach, which are well established in the causal literature to permit numerous false positives and false negatives \cite{Ding2015Mar,Forstmeier2017Nov}. 
Together, our work rigorously defines and studies measurement bias decoupled from demographic confounders in complex, non-Euclidean data.

% In particular, when demographic overlap is enforced \textit{pre hoc} as with \cccombat~(i.e., enforcing demographic overlap prior to the removal of batch effects), we obtain a substantially different answer than when demographic overlap is enforced \textit{post hoc} after the application of competing batch effect correction methods. 
% % what I want to say: Figure 5A top row: we still get a different answer with other stuff
% When we did not enforce demographic overlap \textit{at all} for statistical inference, we still obtain a much different conclusion with both \ccombat~and \Combat~compared to \cccombat.

Together, the conclusions of our work bring into focus the pervasive question of how to properly interpret neuroimaging---or more generally, multi-site scientific---studies.%call into question how the conclusions of neuroimaging mega-studies apply outside of the particular subset of individuals upon which the analysis is performed. 
This concern is at the root of any statistical inference task: a conflict between the internal and external validity. Internal validity refers to how well the conclusions  generalize from the \textit{sample} population to the \textit{study} population; that is, from the observed samples to other samples that are distributed the same as the observed samples.
% the assumed set of individuals from which the \textit{sample} (the individuals who are analyzed during the investigation) is selected. 
External validity refers to the degree to which the conclusions generalize to the \textit{target} population; that is, the population to which we wish to apply the conclusions of our inferences. The target population is typically much broader than the study population, to include, for example, all members of the species, or possibly even multiple species. 

Through simulation, we demonstrated that minor misspecifications of the statistical model employed for batch effect removal can be disastrous in the presence of confounding. Specifically, a minor change of the relationship between the confounder (the covariate) and the measurement from a straight line to a (still monotonic) nonlinearity, the sigmoid, not only inhibited \ccombat~from removing a batch effect, but also introduced an artifact that dwarfed the magnitude of the original batch effect itself. As discussed in the Appendix \ref{app:sim_batch_adj} and expanded on via further simulations, the resulting estimate of the batch effect must, by construction, be considered neither statistically unbiased nor consistent, thus failing two preconditions for internal validity \cite{Westreich2019Feb}. Colloquially, it is almost impossible to know whether models employed for batch effect correction will be reasonable to apply to out-of-sample data (e.g., areas of the covariate distribution for which there is no covariate overlap). That is, we cannot tell whether the data we have will fall into Appendix Figure \ref{fig:sim_lin_adjust} or Appendix Figure \ref{fig:sim_nlin_adjust_nm_sym} and the batch effect correction technique will work reasonably given the underlying true signal, or Figure \ref{fig:sim_nlin_adjust}, Appendix Figure \ref{fig:sim_nlin_adjust_nm_asy}, or Appendix Figure \ref{fig:sim_nlin_adjust_nm_sym_asy} where the assumptions of the batch effect correction technique will be entirely unreasonable. On the other hand, causal strategies can help us to apply far more conservative logic, indicating to us when batch effects are unreasonable to estimate as is often the case with limited overlap in our simulations. Alternatively, they can aid us to first balance the covariate distributions so that we can avoid the pitfalls of the simulations which illustrate poor performance by \ccombat~in moderate or limited overlap regimes. Stated another way, in the absence of knowledge as to the appropriateness of the model employed for batch effect corrections, causal strategies make fewer assumptions about the covariate/outcome relationship to produce reasonable estimates.

Given the major shortcomings theoretically and empirically of traditional approaches for detecting and mitigating batch effects, we therefore chose to first prioritize the development of strategies which sub-sample individuals into demographically-aligned subsets using causal techniques. In so doing, we prioritized internal over external validity, as internal validity is a prerequisite for external validity. Colloquially, one cannot generalize results of an experiment to a target population if the results are, at best, dubiously applicable to the study population. The trade-off of subsampling individuals is that sample size, sample diversity, and  external validity are exchanged for internal validity. We saw this tradeoff materialize dramatically in the CoRR dataset, where over half of all possible dataset pairs could not be assessed for batch effects due to a lack of demographic alignment. This led to a much higher failure to reject rate in the conditional strategy than the adjusted causal strategy for batch effect detection.

The final hurdle of our analysis, therefore, is to address the question: does the internal validity of batch effect removal techniques actually impact downstream statistical inference? A failure to remove batch effects in an internally valid manner, we believe, presents a substantial hurdle for producing internally valid statistical inference downstream, since downstream statistical inference is a product of the corrected measurements. Specifically, one can  endeavor to mitigate batch effects in one of two complementary ways. First, one can filter individuals by ensuring demographic balancing, and then run the entire analysis only on those individuals (as causal approaches, like \cccombat~or Causal NeuroH, do).  Second, one can run an algorithm for batch effect correction (and potentially benefit from a richer number of samples), and then filter to ensure demographic balancing only when measuring downstream demographic-specific effect sizes (as we did by looking for a sex effect only considering individuals from the Raw, \Combat, \ccombat, and Conditional NeuroH~corrected connectomes in the American Clique). If internally valid, causal batch effect correction techniques may substantially restrict the sample size. Perhaps, even if the batch effect correction technique is not internally valid, measurements corrected using it may still have utility for downstream analysis. We anticipated that the most significant features for the inference task would be, at least, ordered similarly between the two analyses. However, our finding that the analyses produce radically different downstream inferences, even when we restrict the non-causal techniques to the same datasets, suggests that there is a link between downstream conclusions and the internal validity of batch effect correction techniques. Stated another way, when we perform batch correction using as few assumptions as possible about the study population through causal techniques, we get extremely disparate downstream scientific conclusions from state-of-the-art conditional techniques. 
This suggests to us that further work is needed to better understand how the removal of batch effects can be achieved while mitigating the deleterious effects on either internal or external validity through careful consideration of the study populations of analyses.

\paragraph*{Limitations and Future Work}

{\edits{The most prominent limitation of our work is that it appears as though we argue in favor of using strategies such as matching to pre-align measurements in terms of the covariate distributions. Indeed, this assumption is extremely restrictive: data will, inevitably, be discarded in the pursuit of internally reasonable estimates. Other techniques we could have leveraged, such as inverse propensity reweighting \cite{Stuart2010Feb}, subvert this limitation, but introduce the related issue that we are still estimating batch effects by upweighting or downweighting particular samples from the data. On the contrary, we do not advocate that the techniques proposed are the \textit{best} approaches one could use for batch effect correction. We posit that the techniques (e.g., prepending matching) or variations thereof (e.g., via inverse propensity reweighting) represent the only context in which naive \Combat, \ccombat, or related strategies leveraging potentially misspecified linear models could be applicable without rigorous justification of linear modeling choices in high-dimensional datasets with non-overlapping covariates as are often encountered in multi-site or multi-experiment mega-studies.

This limitation can be subverted by the development of more robust approaches for estimating the covariate/outcome relationship, building off the NeuroHarmonize approach \cite{Pomponio2020Mar} of incorporating non-linear modeling approaches (such as Generalized Additive Models, or GAMs) into the estimation process. With robust techniques to non-linearities for estimating the covariate/outcome relationship, the restrictive assumption of alignment of the propensity distributions can be exchanged with overlap of the propensity distributions, which informally means that the datasets must have a similar ``range'' of covariates (though, their distributions may differ, which is more realistic for observational data) \cite{Lopez2014}. Informally, you can ``guess'' the covariate/outcome relationship incorrectly and obtain reasonable estimates of batch effects as long as the covariate distributions are smiliar, or you can ``guess'' (or estimate) it correctly and need only covariates to be sufficiently represented in each batch. Unfortunately, estimating non-linear effects in high-dimensionality, low sample-size data (such as neuroimaging connectomes) has proven difficult, and most techniques approach this problem with an assumption of sparsity in the data \cite{Meier2009Dec,Huang2010Aug}, whereas the connectomes of this application (and numerous others, such as in genomics) are often dense. Strategies which can reasonably (explicitly or implicitly) estimate the covariate/outcome relationship provide an exciting avenue for future work.}
%In natively graph-valued data, we may be able to subvert the high-dimensionality, low sample-size problem by better understanding latent representations of the data that are simpler and less complicated. This may be accomplished via spectral methods \cite{Athreya2017Jan,Arroyo2021} or graph neural networks \cite{Xu2020Dec,Fu2020Feb}, in which batch effects could be studied and removed via a sparser representation of the graph data structure.}

Biomedical data science, including neuroimaging and connectomics, are plagued by a lack of generalizability and replicability \cite{Bridgeford2020Dec,Turner2018Jun}. Many investigations posit that this may be a consequence of small sample sizes or overly simplistic datasets \cite{Turner2018Jun,Nee2019Apr} lacking sufficient subject-specific measurements. We argue here that a lack of understanding regarding batch effects, and more fundamentally \textit{causality}, further exacerbates these issues. Indeed, these two perspectives may be considered complementary, in that with insufficient subject-specific measurements both causal and non-causal techniques will run into unmeasured confounding biases. \edits{A limitation of our proposed methods are that their adequacy to overcome confounding pre-supposes the measurement of a rich set of subject-specific covariates, which may often not be the case (and, likely, is not in the CoRR mega study). In this way, our real data investigation serves as illustrative of the wide disparities that can be found when leveraging causal techniques. On the other hand, with sufficient subject-specific covariates (e.g., more than age, biological sex, and continent as a loose proxy for race and culture) we are still likely to run into confounding issues which can be ameliorated by causal techniques such as those proposed herein.}

% what I want to say: Figure 5A bottom row: we get a different answer with causal combat than other stuff
% in this study, we chose to preserve internal over external validity
% We believe the tools and techniques devised in this work will prove critical to achieving these future aims.

%Across the experiments, we tend to find that $Z$-scoring and \Combat\ approaches tend to provide value eliminating disparate undesirable signal from connectomes. We tend to find that all strategies preserve some degree of biological variability in functional connectomes, when the biological variability is known ahead of time. This is evinced by the fact that both \Combat\ and $Z$-scoring show nearly identical detectability of demographic effects before and after correction for batch. What remains an open question, however, is how to best eliminate batch effects in isolation rather than simply removing all site effects (both batch effects and dmeographic effects). Both $Z$-scoring and \Combat\ eliminate all associational batch variability between datasets, even when such variability \textit{should} be present or desirable. For instance, a comparison of the Utah 1 dataset, a dataset containing only a single sex, to any other study indicates no between-batch variability, which is undesirable in cases where two datasets share vastly different demographic characteristics. This suggests that \textit{real biological variability} is being removed by naive \Combat\ correction, which is not ideal for pooled inference. On the other hand, \cccombat\ has preserved a substantial number of adjusted site effects, which also may not necessarily be desirable. It is important to highlight that \Combat\ assumes the data are vector-valued, and does not take careful consideration of the network topology of a connectome. This means that higher-order demographic effects (measured or unmeasured) may persist in the connectomes after removal of site effects. Strategies which adjust the network topology or properties related to the network topology (such as the \textit{latent positions} of a graph \cite{Chung2020Aug}), rather than the edge weights directly, may be more suitable for batch effect removal in connectomes. Future investigations may explore the trade-offs afforded by \Combat\ and novel strategies tailor designed for network-valued data for removing batch or batch effects in connectomics data.

To subvert this limitation, this work suggests that harmonized analyses should be conducted with both harmonized measurement parameters \textit{and} demographic distributions. This represents a departure from the manner in which many mega-studies, including so-called harmonized effects, are currently generated \cite{corr,di2014autism,di2017enhancing,Yamashita2019Apr}, in which potentially demographically-disparate populations are analyzed and only loosely harmonized on the basis of the measurement protocols. We saw that post-hoc batch effect detection and removal (and consequently, downstream statistical inference) presents both theoretical and conceptual inconsistencies with regard to internal validity when not first passed through the lens of causality. {\edits{This problem is highlighted by looking at the poor demographic overlap for popular neuroimaging studies such as CoRR in Figure \ref{fig:demographic}, and in the ABIDE study \cite{di2014autism,di2017enhancing} detailed in Appendix Figure \ref{fig:overlap2}. While the SRBPS \cite{Yamashita2019Apr} has greater overlap on the basis of age, sex, and likely race (same continent) for many pairs of datasets (but still has two datasets that are dissimilar from the others), both ABIDE and SRBPS add the additional hurdle of neurodivergences being used in participant recruitment to batches. If the connectome is a proxy for brain function, neurodivergent brain features captured by the connectome may cause symptoms of neurodivergence \cite{Vogelstein2019Apr,Konrad2010Jun,Hashem2020}, introducing a potential causal mediation effect if the neurodivergence is leveraged directly in participant recruitment \cite{Pearl2014Dec,Bareinboim2012Mar}. While this issue has been explored for the wider problem of data fusion \cite{Bareinboim2016Jul}, is unclear the extent to which causal mediation impacts batch effect detection or correction in high-dimensional biological studies.}}

Future work will focus on applying the methods developed here to harmonize mega-studies such as the Adolescent Brain Cognitive Development (ABCD) \cite{Karcher2021Jan}, which includes $N>12,000$ demographically diverse individuals from across the United States, and measurements were produced with a consistent, harmonized measurement protocol. Further, future studies may seek to build upon the NKI-RS and the recent work by \citet{Noble2017Feb} by collecting measurements from the same expansive and diverse group of people across multiple neuroimaging sites or scanning protocols. Our work provides a theoretical foundation for evaluating existing neuroimaging studies, such as \citet{Noble2017Feb} and \cite{Yamashita2019Apr}, which advocate for the aggregation of studies featuring individuals measured at a wide array of sites (so called \textit{travelling subject} datasets), so that the nature of batch effects can be explored with minimal assumptions needed due to the crossover property. In such datasets, scientists can appreciate internally valid demographic-specific effects on the measurement, and learn more reasonable modeling assumptions, so that perhaps extrapolatory pre- or post-hoc batch effect removal techniques (such as in Appendix Figure \ref{fig:sim_lin_adjust}) can be employed to mitigate the batch effects in other non-demographic-aligned datasets. Failure to attain a comprehensive understanding of batch effects and the extent to which they pervade statistical inferences, we believe, calls into question the validity of existing results in scientific meta-analyses. 

\section*{Code Availability}

The code used for analysis within this manuscript is available at \\ \href{https://github.com/ebridge2/batch_effects}{github.com/ebridge2/batch\_effects}, and a docker container with all software versions is available at \href{https://hub.docker.com/r/neurodata/batch_effects}{hub.docker.com/r/neurodata/batch\_effects}.

\section*{Data Availability}

The raw data analyzed in this manuscript can be obtained at \href{http://fcon_1000.projects.nitrc.org/indi/CoRR/html/}{CoRR Mega-Study}. The pre-processed data analyzed in this manuscript is available at \href{https://neurodata.io/mri/}{neurodata.io/mri}.

\section*{Contributions}

EWB wrote the manuscript; EWB and JTV revised the manuscript; EWB, JTV, and MP conducted study conception and design; EWB, JTV, BC, and MP conducted interpretation of results; EWB, JC, SP, and RL analyzed raw neuroimaging and demographic data; EWB and JTV devised the statistical methods; EWB analyzed processed data; MP, GK, SN, TX, MM, BC, and JTV provided valuable feedback and discussion throughout the work.

\subsection*{Acknowledgements}
% UPenn, NSF Career Award, Duke
% add Stephanie, Betsy
The authors are grateful for the support from the National Science Foundation (NSF) administered through NSF Career Award NSF 17-537, the National Institute of Health (NIH) through National Institute of Mental Health (NIMH) Research Project 1R01MH120482-01, and the NIH through Research Project RO1AG066184-01. We also wanted to thank Betsy Ogburn for numerous insightful conversations regarding the presentation and framing of this work. 
% TODO@jv: who is duke PI?